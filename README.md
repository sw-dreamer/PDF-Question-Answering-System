# AI 기반 PDF 문서 질의응답 시스템입니다.

PDF를 업로드하고 한국어로 질문하면 문서 내용을 분석하여 답변해줍니다.

## 주요 기능
- PDF 업로드 및 텍스트 추출: PDFPlumber를 사용한 정확한 텍스트 추출
- 벡터 검색: Sentence Transformers를 이용한 의미 기반 문서 검색
- QLoRA 최적화: 4-bit 양자화로 메모리 효율적인 LLM 추론
- 한국어 특화: Gemma-Ko 모델 기반 한국어 질의응답

## 프로젝트 구조
```
src/
├── app/
│   ├── __init__.py
│   ├── main.py                    # FastAPI 앱 진입점
│   ├── routes/
│   │   ├── __init__.py
│   │   ├── upload.py             # PDF 업로드 엔드포인트
│   │   └── query.py              # 질의응답 엔드포인트
│   └── services/
│       ├── __init__.py
│       ├── pdf_service.py        # PDF 텍스트 추출
│       ├── vector_service.py     # 벡터 검색 서비스
│       ├── tinyllama_service.py  # QLoRA LLM 서비스
│       └── chunker.py            # 텍스트 청킹
└──  requirements.txt
```

## 파인튜닝 기법 비교

| 기법 | 학습 파라미터 | 메모리 사용 | 학습 시간 | 성능 | 사용 사례 |
|------|---------------|------------|-----------|------|-----------|
| Full Fine-tuning | 100% | 매우 높음 | 매우 느림 | 최고 | 대규모 데이터셋 |
| LoRA | ~0.1-1% | 낮음 | 빠름 | 높음 | 일반적인 파인튜닝 |
| QLoRA | ~0.1-1% | 매우 낮음 | 빠름 | 높음 | GPU 메모리 부족 시 (현재 사용) |
| Prefix Tuning | ~0.1% | 낮음 | 빠름 | 중간 | 프롬프트 최적화 |
| Adapter | ~1-3% | 중간 | 중간 | 중간 | 멀티태스크 학습 |

---

### 1. Full Fine-tuning (전체 파인튜닝)

**개념:** 모델의 모든 파라미터를 업데이트  

**장점:**
- 최고의 성능
- 완전한 모델 커스터마이징

**단점:**
- 막대한 GPU 메모리 필요 (Gemma-2B: ~20GB)
- 학습 시간이 매우 오래 걸림
- Overfitting 위험

**적합한 경우:**
- 대규모 데이터셋 (10만+ 샘플)
- 충분한 컴퓨팅 자원
- 완전히 새로운 도메인/언어

---

### 2. LoRA (Low-Rank Adaptation)

**개념:** 모델의 가중치 행렬에 작은 어댑터 추가  

**장점:**
- 파라미터 0.1~1%만 학습
- 메모리 효율적 (~8GB)
- 여러 어댑터 교체 가능

**단점:**
- Full Fine-tuning보다 성능 약간 낮음
- 하이퍼파라미터 조정 필요

**적합한 경우:**
- 중소규모 데이터셋 (1천~10만 샘플)
- GPU 메모리 제한적 (8-16GB)
- 여러 태스크 학습

---

### 3. QLoRA (Quantized LoRA)

**개념:** LoRA + 4-bit 양자화  

**장점:**
- 극도로 낮은 메모리 사용 (~4GB)
- LoRA와 거의 동일한 성능
- 소비자용 GPU에서 대형 모델 학습 가능

**단점:**
- 양자화로 인한 미세한 성능 저하
- bitsandbytes 라이브러리 의존

**적합한 경우:**
- GPU 메모리 부족 (4-8GB)
- 빠른 프로토타이핑
- 개인 프로젝트

#### 파이튜닝 효과 비교

| 시나리오 | 양자화만 | QLoRA 파인튜닝 | 효과 비교 |
|-----------|-----------|----------------|-----------|
| 성능 향상 | 40-60% | 80-95% | +40% |
| 답변 형식 일관성 | 낮음 | 높음 | +50% |
| 도메인 특화 용어 이해 | 약함 | 강함 | +60% |
| 프롬프트 민감도 | 매우 높음 | 낮음 | 안정화 |

---

### 4. Prefix Tuning

**개념:** 입력 앞에 학습 가능한 프리픽스 토큰 추가  

**장점:**
- 극소량 파라미터 (~0.1%)
- 매우 빠른 학습
- 프롬프트 엔지니어링 자동화

**단점:**
- 성능이 LoRA보다 낮음
- 복잡한 태스크에 부적합

**적합한 경우:**
- 프롬프트 최적화
- 간단한 분류/생성 태스크
- 초고속 학습 필요

---

### 5. Adapter

**개념:** 각 레이어에 작은 병목 네트워크 추가  

**장점:**
- 모듈식 설계
- 멀티태스크 학습 용이
- 어댑터 조합 가능

**단점:**
- LoRA보다 파라미터 많음
- 추론 속도 약간 느림

**적합한 경우:**
- 여러 도메인 동시 학습
- 태스크별 스위칭 필요


---
# 결과

<img width="1144" height="784" alt="image" src="https://github.com/user-attachments/assets/e4df0880-99d3-4864-b5b3-1655f1743728" />

<img width="1155" height="883" alt="image" src="https://github.com/user-attachments/assets/e41959ac-8f66-43c9-a98a-1e963898e664" />
